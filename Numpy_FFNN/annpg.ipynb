{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 23:07:58.663627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/9nrqpdg94f9976b246v2q0jm0000gn/T/ipykernel_8171/3154918458.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "# Usage example:\n",
    "train_X = read_idx('../Data/mnist/train-images-idx3-ubyte')\n",
    "train_y = read_idx('../Data/mnist/train-labels-idx1-ubyte')\n",
    "test_X = read_idx('../Data/mnist/t10k-images-idx3-ubyte')\n",
    "test_y = read_idx('../Data/mnist/t10k-labels-idx1-ubyte')\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8362167-5189-44e2-9def-3277bc4e46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "num_pixels = train_X.shape[1]*train_X.shape[2]\n",
    "# flatten X\n",
    "train_X = train_X.reshape(train_X.shape[0], num_pixels).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], num_pixels).astype('float32')\n",
    "# one hot categorical Y\n",
    "train_hot = np.eye(np.max(train_y)+1)[train_y].astype('float32')\n",
    "test_hot = np.eye(np.max(test_y)+1)[test_y].astype('float32')\n",
    "# lets put these back together so that we can actually shuffle them later (normalize too)\n",
    "X = np.concatenate((train_X, test_X), axis=0)/255\n",
    "Y = np.concatenate((train_hot, test_hot), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e97820b3-69e1-4122-baef-a85c9a099ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 70000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxsAAACvCAYAAACVbcM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAa+0lEQVR4nO3de3RU9bXA8T0gmCAE5CEXNIuHCVAQxGYQ5YKUEkGxxaIBbOURwQLiAnIFQRAbnkrFB1KuLRQqyuNCCBUVqy6pYKEQZcJDBFEkhaKgvITwSgLJ3D/uknXP7J9yGOaXk5n5ftbqH7/t75zZkMOZ7M7ss33BYDAoAAAAABBhlbxOAAAAAEBsotgAAAAAYAXFBgAAAAArKDYAAAAAWEGxAQAAAMAKig0AAAAAVlBsAAAAALCCYgMAAACAFRQbAAAAAKyg2AAAAABgBcWGS/n5+XLXXXdJUlKS1KhRQ7p16ybbtm3zOi3EieLiYhk3bpw0bNhQEhMTpX379vL+++97nRbixLp168Tn8xn/l5eX53V6iHE7d+6U3r17S9OmTaVatWpSt25dueOOO+Stt97yOjXEgczMzB+8//l8Pvn666+9TrHCu8rrBKLBli1bpGPHjpKcnCzZ2dlSVlYmL7/8snTu3Fk+/vhjad68udcpIsZlZmZKbm6uZGVlSWpqqixcuFB69Ogha9eulY4dO3qdHuLEyJEjpV27do5YSkqKR9kgXuzfv19OnTolAwcOlIYNG8rZs2dl5cqV0rNnT5k7d64MGTLE6xQRw4YOHSrp6emOWDAYlGHDhknjxo3l+uuv9yiz6OELBoNBr5Oo6O655x7ZtGmT7NmzR+rUqSMiIocOHZJmzZpJt27dZOXKlR5niFj28ccfS/v27WXmzJkyZswYEREpKiqSm266Sa677jrZuHGjxxki1q1bt066dOkiK1askIyMDK/TAaS0tFTS0tKkqKhIdu/e7XU6iDMbNmyQTp06yfTp02XChAlep1Ph8TUqF9avXy/p6ekXCw0RkQYNGkjnzp1l9erVcvr0aQ+zQ6zLzc2VypUrO/7fu4SEBBk8eLBs2rRJDhw44GF2iDenTp2SCxcueJ0G4lzlypUlOTlZTpw44XUqiENLly4Vn88nv/nNb7xOJSpQbLhQXFwsiYmJKl6tWjUpKSmRTz/91IOsEC+2bt0qzZo1k6SkJEf81ltvFRGhdwjl5qGHHpKkpCRJSEiQLl26SCAQ8DolxJEzZ87I0aNHZe/evfLiiy/KO++8I127dvU6LcSZ8+fPS05OjnTo0EEaN27sdTpRgZ4NF5o3by55eXlSWloqlStXFhGRkpIS+eijj0REaA6CVYcOHZIGDRqo+PexgwcPlndKiDNVq1aV+++/X3r06CF169aVXbt2yXPPPSedOnWSjRs3yi233OJ1iogDo0ePlrlz54qISKVKleS+++6TOXPmeJwV4s17770nx44dkwcffNDrVKIGxYYLw4cPl0ceeUQGDx4sY8eOlbKyMpk2bZocOnRIRETOnTvncYaIZefOnZOrr75axRMSEi7+d8CmDh06SIcOHS6ue/bsKRkZGdKmTRsZP368vPvuux5mh3iRlZUlGRkZcvDgQcnJyZHS0lIpKSnxOi3EmaVLl0qVKlWkT58+XqcSNfgalQvDhg2TCRMmyNKlS6VVq1bSunVr2bt3r4wdO1ZERKpXr+5xhohliYmJUlxcrOJFRUUX/ztQ3lJSUuTee++VtWvXSmlpqdfpIA60aNFC0tPTZcCAARf7JX/5y18Kz7lBeTl9+rS88cYb0r17d0cfL34cxYZL06dPl2+//VbWr18vn3zyiWzevFnKyspERKRZs2YeZ4dY1qBBg4ufov1/38caNmxY3ikBIiKSnJwsJSUlcubMGa9TQRzKyMiQzZs3yxdffOF1KogTq1atkrNnz/IVqsvE16guw7XXXuuYabBmzRq54YYbpEWLFh5mhVjXtm1bWbt2rRQWFjqaxL/vGWrbtq1HmSHeFRQUSEJCAp/uwhPff4X05MmTHmeCeLFkyRKpXr269OzZ0+tUogqfbIRp+fLlsnnzZsnKypJKlfhrhD0ZGRlSWloq8+bNuxgrLi6WV155Rdq3by/JyckeZod4cOTIERXbvn27vPnmm9KtWzfugbDq8OHDKnb+/Hl57bXXJDExUVq2bOlBVog3R44ckTVr1kivXr2kWrVqXqcTVfhkw4V//OMfMmXKFOnWrZvUqVNH8vLy5JVXXpG77rpLRo0a5XV6iHHt27eX3r17y/jx4+Xw4cOSkpIir776quzbt08WLFjgdXqIA3379pXExETp0KGDXHfddbJr1y6ZN2+eVKtWTWbMmOF1eohxQ4cOlcLCQrnjjjvk+uuvl2+++UaWLFkiu3fvlueff55P1lAuli9fLhcuXOArVGFggrgLe/fuleHDh8uWLVvk1KlT0qRJExk4cKA89thjUrVqVa/TQxwoKiqSp556ShYvXizfffedtGnTRqZOnSrdu3f3OjXEgdmzZ8uSJUvkyy+/lMLCQqlXr5507dpVsrOzJSUlxev0EOOWLVsmCxYskB07dsixY8ekRo0akpaWJiNGjODrLCg3t99+uxQUFMjBgwcvjkGAOxQbAAAAAKzgi7YAAAAArKDYAAAAAGAFxQYAAAAAKyg2AAAAAFhBsQEAAADACooNAAAAAFa4Hurn8/ls5oEoVV5PTub6g0l5PrmbaxAm3APhJa4/eMnt9ccnGwAAAACsoNgAAAAAYAXFBgAAAAArKDYAAAAAWEGxAQAAAMAKig0AAAAAVlBsAAAAALCCYgMAAACAFRQbAAAAAKyg2AAAAABgBcUGAAAAACsoNgAAAABYQbEBAAAAwAqKDQAAAABWUGwAAAAAsIJiAwAAAIAVFBsAAAAArKDYAAAAAGAFxQYAAAAAKyg2AAAAAFhBsQEAAADACooNAAAAAFZQbAAAAACw4iqvEwBwebJllopNPp2lYsUHnOsdZwJqz0/Fr2KV/H83vGpXl9kBiFXjZIaKzZj7hIr9e6g+tlHtvTqYc6NzfaPeIsd0KGg41eG+OnYg4LznNWqq73eDai9Usbck05AIgHDxyQYAAAAAKyg2AAAAAFhBsQEAAADACooNAAAAAFb4gsFg0NVGn892LlZNkGdV7OkFdzoDnW9Rew7V0eeqaWhYS0jRsUofhQTabzdkdrMhFj1cXj5XLNqvv7DN36FCxQ+3VrEdubmRe81161Qo+Pwcx3rl1b9Xe2bIuMjl4FJ5XX8icXwN4kfF8j0w9GEUkz7MUnvyjxjuPSdOhPeCq1apkL+ffl+WB6a6Ol0g9L5oyMuf+rA+sLOr01cIsXz9oeJze/3xyQYAAAAAKyg2AAAAAFhBsQEAAADAiqgf6tdeFqnYE8H+KtbCN1bF7mu0xrFOu0mf36fnoBmHDMmHOlSc5FzvmLhC7fG3M5zrXkMMMSdDBjrWue+9qvYE1i9XsYj2Zxg0mjNHxfb/zPmaz3yYoQ/sXDH6OAD8uP4yXMUWLe6lYtktnH2N+dvm65PV0qG0ZN0HseHnel+nZ0Km863upDetbq5jEbS+g44ZsoAHCr9yrr/4Rv9Clva5HtToq6nP9eQvnlOx6TIm7NxwefhkAwAAAIAVFBsAAAAArKDYAAAAAGAFxQYAAAAAK6JrqN+CrSq0Y7Ae+FO8Zo2KvdM1XcWequZsTrvt3CC1J8/U+e2SagBeOEztCabermK+zL0qJntuDDsPmxgo5NYbKrK21PkkgBqv2238NkmroRu9p3SfrWKDZKRj/a2hSd1fdlq/QJ/MsHNzg6F+cSQ75L6YWqD3+C7oWFA/ByW7304VmyxZYaUVjffA40d1rGCVofnbBX/wP1Us47czVCxX9AMwwlVJ9IMzZgX7qliHlS6G+h3sol/gdxXz/dYkGq8/t4o/c6537LyC90jDsFr/sJCHoeSHf/oK4Xyyjj18wOpLMtQPAAAAgKcoNgAAAABYQbEBAAAAwAqKDQAAAABWVJgG8YnyvIrlB0c71lOfmqj2/PRX01Sskv89FUsUPR31nJy9nBStmCDPqtjTS1rrjQ/e7VjWlnpqy3H5k4rVFt2UflyOXEaGPy6Wm9PClS2zVGyiofl0u+VJ4Mq2bSrkb6z//Yge/isy3dmcGzj3ij7XCMO56rvMLUw0iEeWqfG27M+PqNjB3x5XscphvmapIWY614GOHZ2BzEy9qVYtHTM0Bdd8WF/kqatDAr8wJGEQlffAD3QocFzfj9LqOR8g4Xvd8PCSWbabqf+pIkeO66b0/R8Y7qf79jmW/haGidEuf84VVVRef26FPhRikL7WAvfcpI+bNMlOPlHIX/ikDg76PGLnp0EcAAAAgKcoNgAAAABYQbEBAAAAwAqKDQAAAABWeNIgniotVWzPyV0qFtjsnATur6GngEv7iKUVXXKeUaFA7XYq9m66bgSaKI9GLI2Ybk5zwdQM3t/QDH7C0AyetjNkerehzy0/6K6JPG2ungSeP9R5rP8evUcSXZ1eOW+ImVrORhqmBn8gA8N7UQMaxM3GiZ7g/PuchioWHDDAsc5f7PKhBS4bsWXUKMcy+euv1Ra3DeL1Pw0JbKqtNwXP6JjvGsPZDHZudq5dNj5H4z2wvwxXsUWLeqtYdv/tjnW4U9Z/yBSZ61jfWTJU7WlaVR93YL67aef+4pDp4I9Gz2Rwt6Lx+osk08OFpk3/ld44Sv/st1d3rs+X90NbyoG/0PC+Pyhy56dBHAAAAICnKDYAAAAAWEGxAQAAAMAKig0AAAAAVnjSID5dXlax7muaqVj9ds6G8JtrmqZmR24adjQ5+6WO7dq3RsVoEI+c26Szis08/6GKJb6hm8zqZ+gmrWTJd6w/lzS155TbhrWQSbkiIjNHO6fltvTNVnuyZaS784cwNYibJqL7M5INOyP3VAcaxEVk4SYVCu6/XcXyW+mfT+WQ67Lte/r0+VM7qlja+A0q5jttyC1k8rx8QoNuuCrs9WdgenDG5L9mqdjJ+5zrPab7nenBAwZpVfRUeF/l952Bz5vqA6dG9zXJ9Re+qTLHsb5Krir3HB4+PkzF9n/g4n1/1iwV8r+t78tZNbP1oTLZVW5u0CAOAAAAwFMUGwAAAACsoNgAAAAAYIUnPRuS80cVCtROVbE+6d861gXyYORyiDJtxDmwb5VsVnvOGo5bKs+q2NMyNlJpxdf3RRd2UqFA9VGGjdrkDP2F9pFlmY71tX91OcDvmO7/8F3YqzdaHGBFz0b5CB2+lhjU/W6PGNIqNQw9GzhYf6d9p+/vIZGuhixM4xqbG2LxKa7ugSZ/06E9PXTspJtBfG6HRboVej7D99yTN+jvuZvUn3e1Y11pyGtqT5n0dZtZxMT99VeBhfYujf4qS+35okV1FZOFCy957lat9e8BiR7clunZAAAAAOApig0AAAAAVlBsAAAAALCCYgMAAACAFeU/wURERE652tUqpA21wEYqUeIGaexYH1/zjNrjr5WuYuKPXDN43OnvbLoODtMN1/lF61XM/6Ju1v6dvKFi9UJK/QuG/vCN9+smsBm+gXqjRPdgKpi9dNTZEF6wQDfZfhLUjd8DfXoAquhtYm4ID0UzOH5YoKoeJnsy94SrY30hQyUbGPZUNsTOFupYtSQdqx/SvP6tu15w+cYQCzR927GuJ/r9ttH/PKZiqb9+V8X2yC53iSBqTJKXVGz8Z84HyOzIM7zJL3R3fn+p89/Kk82fM+waY4hVDHyyAQAAAMAKig0AAAAAVlBsAAAAALCCYgMAAACAFd5MEF/8vgoF/kOf37+liTMwNo6bYJe97lgG6tZQW/zpOhbJac0msTy9NDjXuc6vY5iQ3U43cEsjHXpSdDNXr9zGjvVP0vS5rmmiQhWCaYK4yVR5QcWmiG6iDFesTxAPBAKOdT2/X+0xNaVKPf33Pih9gYqdl3zHepHoCeX4cbF8D3QjqC8rkak61ONfOvaOLz8kkhaJlK5YJVmuYqVvO6eD775HH3fGMCXdX32/3viA4S8oTPF+/VUYOc+rUKCS4ZeBEKEPSRARSctZomIT+zgfWzBNRl9GcvYwQRwAAACApyg2AAAAAFhBsQEAAADACooNAAAAAFZ4MkF8Yr9PVKyKodkluNK5TjZ03n4lhuarGDS1by/HOtHQq/U7YzMu3Ljb0OOUHzKtuXUr3ciV2qilipmmw07P0XNwe4Wsr0n48Rw9leqcir69vp5iPnC9Hs+70xe5ZvB45C/4yLH+zLAncEZfg3JGN6pKbk0VSqvhbAgf3sVdXqOr6NidvlkqNlmy3J0QUevng3uo2B3STcXe8WUZjq4YDeGhyqSvivlCGsI3lejjqrymY/sNzeCXbhtGRdbK8PvCYp/+HbY01zAxPETD7wzBax9UoWluEqvA+GQDAAAAgBUUGwAAAACsoNgAAAAAYAXFBgAAAAArvJkgbpKjx5AGmt7sWNdppqfnDk56VcU+EN28WlENkcdVbN6GmSoWeHeiY+1vbGgXejhiabkWldNLT+pQ4H3dyJU2zNkQ3vPoQrXnLcl09ZJTDX9Nd690vqb/Pw3TyBu4On1EZUm2ir0gkx3rLYbGN3/ZaX2yPpmRSsso1ieIhzJNNi5bvNrVsev7LrrknrqGxm/TVGSpVUuFQv+9iIg8cMS5ruobrvZE+9TyqLwH4oqZG8T1v5W0KvqN2RfBX1G4/izLWaZCW/s8oGJumsF/kqbvkc82maVi0fRgDSaIAwAAAPAUxQYAAAAAKyg2AAAAAFjhyVA/k/598lXMv3SwYx3Ypr8POd/Qn/Go4Stk7/j2qtg4WeFY/16euFSal2Wc/Nl5/v/+udoz9+iNKjakaI0+WeN9zvV8/ecR0eeCQVV323xPX+0MDMkM+yWfWrxWxe5OPOYMvFwxfqazugxQsX6Ptg7ZNEvtmbbh1yo2UUVwJUzDxqSfu2M7udg3Veao2GFDL9gfcopVzPTN3cdDBmPqDjsgOqSKc3hm80I9vLXA0MtUxtty9Fi+QoW29umtYm76M0R0j8bMJi+pPZNllMvkohufbAAAAACwgmIDAAAAgBUUGwAAAACsoNgAAAAAYEXFGernRhdDA+1C3X0VaFVd71u1SoVqpKc71rXDzesH/DtknXJI76m5zPBnGqH/TN+GtPLXDz+tiIrKgULndCjwtm74ej1jn2M9XcaE/ZI9RDeZ/S3nLsf6hj5T1J6vZH/Yr+lGf9FD1l7er4esfb45ZABh2QV9MsOgI9vibahfhXabvpcFxmx1rP1tDYMrU2wlVD6i8h6IyzZInIOHh89393P3dzI8YaF5JDL6P1x/VyCkITy/r36fDobZDC6iG8InxWAzOEP9AAAAAHiKYgMAAACAFRQbAAAAAKyg2AAAAABgRXQ1iLt0m3RWsVRppWKLFveK3Iv20+3lXeQpxzpP1qk9/yWTVGyojFWxRqGjd/XgdE9EZXOaywZxf1FNZ6DfnZHLoaL4i+5UDCRNV7HqGc7mt9flT2rPeBkWubxcokG8AtmgQ4Hdzgni/oIuetPT0T1iOSrvgbhsSSE/5g8WzDdvDOEfYbjmz0Xumuf6c2eiPK9ij8tox3pPBJvBRWKzITwUDeIAAAAAPEWxAQAAAMAKig0AAAAAVlBsAAAAALAiJhvEo8psw9Tdlv9SMf+BkGblh9rZyuiyRGVzmssG8VO9nE1g4yvrBw/kyYcRSyvSQqeWv3Rqhdpz8u6OKpa4QXf6tqodcp0erxhNvTSIeyRntgp91mekijX+3Lnu0jy6/g25EZX3QJcqyXLHuuzPj+hNvz1eTtmUJ8P7cu5WZ+DECbXn+vv1tPB21zZSsa9kf9iZhYrl6y9cT8pzKjbhX2NU7LN8Fw3hGboZ3C87DRv1Q4jiAQ3iAAAAADxFsQEAAADACooNAAAAAFZc5XUCca+a/u77Vel6W5ZkO9azLKUTF17Q38et8qT+OdQIGfDTKKi/u5k3QJ9LFnnQz7BorQq9XdU5TCr/Pf391Oob9J+pRWh/hkiF6dFAhBWHrPfoLcEHdCy/UkMVq244fWLzv4dEurrNDBVA2WLnsNpAwjy1J+0jfQ/xrdb3kEFT113y9f4ig92m5kqqtHSs99zwlt60Xt/b8pvoWLDWGsf6GsN3+Rsk6cFxEjI4DvZNz6msYr3c9GdkZqqQP2eJ3tcnPvszrgSfbAAAAACwgmIDAAAAgBUUGwAAAACsoNgAAAAAYAVD/TxW8qmOfdhSx+6soGVhzAwUWthJhQKBm52Bn/1MH7dtmwr5s6bpfav1+eXCy851aWu951YdOn+Ljm3PvXTzW51uuqFxWdKfVGy8DLvkuSoKhvpdmU0lznWVN/V15DM0wv7T8Nee5VuuYmXSN+zcokXM3AMNRsh8x3p2fz20Lv9ew73HMPDOjfMD9PlvP2bY2MvwEItHH1eh43f/1bEuWDVf7XHrJ12duT3RRJ/rD6Lzty2Wrz83hoj+uc88NFPF9vwzvPfIJknh5RUvGOoHAAAAwFMUGwAAAACsoNgAAAAAYAXFBgAAAAAraBAvR7WlnoodW35ExXynDAeXf9+ZKzHdnJazzLHc9Cs9SrlRVX3YQRfN2pHW3NDEW+NV5/q2gZ3Vnjz50FZK5YIG8SvUJqTR9vF/6z39TR2SaVbSiUYxfQ8M8YwYHigx/U4VWz9WT+DuOFufLz87ZO78woV6U5jN5iIikp3tWJ7b97Xa0mmj4bgRhgb0T/SfqSKIp+vPpPArHfsiL7z3YH/ZQR3sMzKsc8ULGsQBAAAAeIpiAwAAAIAVFBsAAAAArKDYAAAAAGAFDeLl6El5TsV6rWmrYv596fpgGsTL5XUuV1NZomKBow+qWMG6Szespb2hm7x9fzRsXPFTFZr00EAdk1GXfM1oR4M4vBbv90C3xskMFasltRzr8V8O0weWhv+azzR3NrSPF8P5o1y8X387DbFzYT6kxV+k34OlX1inihs0iAMAAADwFMUGAAAAACsoNgAAAABYQbEBAAAAwAoaxMuRaYJ4fqGeIN4kx3AwDeLl8jqILjSIw2vcA+GleL/+rqhBfNUqx9J/z3V6z69fuOyc4gkN4gAAAAA8RbEBAAAAwAqKDQAAAABWXOV1AvHkuBj6M5I8SAQAACBOpO3UA/smLZ7lDCzOKpdc4hGfbAAAAACwgmIDAAAAgBUUGwAAAACsoNgAAAAAYAVD/XBF4n2gELzFUD94jXsgvMT1By8x1A8AAACApyg2AAAAAFhBsQEAAADACooNAAAAAFa4bhAHAAAAgMvBJxsAAAAArKDYAAAAAGAFxQYAAAAAKyg2AAAAAFhBsQEAAADACooNAAAAAFZQbAAAAACwgmIDAAAAgBUUGwAAAACs+F/v8SDiy9SxvgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = X.shape[0]\n",
    "print(f'm = {m}')\n",
    "n = 5\n",
    "fig,axs = plt.subplots(1, n, figsize=(10,2),dpi=100)\n",
    "for i in range(n):\n",
    "    rnd = np.random.randint(m)\n",
    "    # plotting first few images from the dataset\n",
    "    _=axs[i].imshow(X[rnd].reshape(28,28), cmap=plt.get_cmap('nipy_spectral'))\n",
    "    _=axs[i].set_title(f'{np.argmax(Y[rnd])}')\n",
    "    _=axs[i].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa68e0c-bcf9-4f88-9436-bd19c5b2c646",
   "metadata": {},
   "source": [
    "<center><u>components of the gradient</u></center>\n",
    "<br>\n",
    "<center>regression predicts variables $w, b$</center>\n",
    "\n",
    "<center>$x, y$ are constants</center>\n",
    "<br>\n",
    "<center><u>logistic regression process</u></center>\n",
    "\n",
    "$$Z = W^TX + B$$\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "$$A_{outputs} = \\hat{Y}$$\n",
    "\n",
    "$$L = L(\\hat{Y}, Y) = L(A_{out}, y)$$\n",
    "\n",
    "$$L\\ = - y \\log{(A_{out})}\\ +\\ (1 - y)\\log{(1 - A_{out})}$$\n",
    "\n",
    "<center>so we need the output of the forward pass i.e.  \n",
    "\n",
    "$$\\hat{y}$$\n",
    "\n",
    "to compute the derivative of the loss function</center>\n",
    "\n",
    "<center><u>Chain Rule</u></center>\n",
    "\n",
    "$$Z \\rightarrow A \\rightarrow L$$\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + \\ldots + b$$\n",
    "\n",
    "$$dw_1 =  \\frac{dL}{dw_1} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{w_1}}$$\n",
    "\n",
    "$$ = x_1 \\cdot \\frac{\\partial{z}}{\\partial{w_1}}$$\n",
    "\n",
    "$$dw_2 =  \\frac{dL}{dw_2} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{w_2}}$$\n",
    "\n",
    "$$ = x_2 \\cdot \\frac{\\partial{z}}{\\partial{w_2}}$$\n",
    "\n",
    "$$dw_b =  \\frac{dL}{db} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{b}}$$\n",
    "\n",
    "$$ ? = \\frac{\\partial{z}}{\\partial{b}}$$\n",
    "\n",
    "***\n",
    "\n",
    "$$\\frac{\\partial{z}}{\\partial{w_1}} + \\frac{\\partial{z}}{\\partial{w_2}} + \\ldots \\frac{\\partial{z}}{\\partial{w_m}} + \\frac{\\partial{z}}{\\partial{b}} = dZ$$\n",
    "\n",
    "$$dz = \\frac{\\partial{L}}{\\partial{z}} = \\frac{\\partial{L}}{\\partial{a}} \\cdot \\frac{\\partial{a}}{\\partial{z}}$$\n",
    "\n",
    "$$ = \\bigl( - \\frac{y}{a} + \\frac{1-y}{1-a}\\bigr) \\cdot a(1-a)$$\n",
    "\n",
    "$$ = a - y$$\n",
    "\n",
    "***\n",
    "\n",
    "$$da = \\frac{\\partial{L}}{\\partial{a}} = - \\frac{y}{a} + \\frac{1-y}{1-a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07c680fa-0c74-427b-843f-b78a2ca5ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann():\n",
    "    # sizes are a list describing the dense layers\n",
    "    # the itnput size and output size for now will be inferred\n",
    "    # from the size of the X and Y data\n",
    "    # likely  could implement something where itnput and output layer are  not  produced  \n",
    "    # until data is offered, im not sure if this is a bad idea. ill have to research a bit.\n",
    "    # data examples are expected to be provided as rows\n",
    "    def __init__(self, sizes=None, epochs=1, rate=0.001, X=None, Y=None):\n",
    "        self.sizes = None\n",
    "        self.num_layers = None\n",
    "        self.m = None\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.J = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.batch_size=0\n",
    "        self.times = []\n",
    "        self.test_run = False\n",
    "        self.split_data = None\n",
    "        self.foreign_split = False\n",
    "        self.epochs = epochs\n",
    "        self.lr = rate\n",
    "        self.params = self.build_init(sizes)\n",
    "        self.batch_d = None\n",
    "    \n",
    "    def build_init(self, sizes:list=None, acv_hidden:object=None, acv_output:object=None, overwrite=False):\n",
    "        if self.sizes is None:\n",
    "            if sizes is None:\n",
    "                print('error: no sizes to create layer params, please give sizes.')\n",
    "                return\n",
    "        else:\n",
    "            if sizes is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite build without overwrite flag,\\n\\\n",
    "                                please  set flag if you want to rebuild.')\n",
    "                    return\n",
    "                else:\n",
    "                    self.sizes = sizes\n",
    "                    params = self.params # not really sure what i am doing here\n",
    "        self.sizes = sizes\n",
    "        params = self.params = {}\n",
    "        for i in range(1,len(self.sizes)):\n",
    "            params['W'+str(i)] = tnp.random.uniform(-1,1,(self.sizes[i-1], self.sizes[i]))\n",
    "            params['dW'+str(i)] = tnp.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "            params['B'+str(i)] = tnp.zeros((1,self.sizes[i]))\n",
    "            params['dB'+str(i)] = tnp.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "        return params\n",
    "    \n",
    "    # activation funtion for hidden layer\n",
    "    # logistic function\n",
    "    def sigmoid(self, z):\n",
    "        A = 1./(1+tnp.exp(-z))\n",
    "        return A\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return (self.sigmoid(z) * (1 - self.sigmoid(z)))\n",
    "        \n",
    "    # activation function for output layer\n",
    "    # softmax is the probability function density with exponentiation\n",
    "    def softmax(self, z):\n",
    "        # normalize to stabilize for large exponentials\n",
    "        zn =  z - tf.reduce_max(z, axis=-1, keepdims=True)\n",
    "        # exponentiate\n",
    "        zne = tnp.exp(zn)\n",
    "        # sum\n",
    "        znes = tnp.sum(zne, axis=-1, keepdims=True)\n",
    "        # learned to add this constant in  csc421\n",
    "        return zne / znes  + 1e-15\n",
    "    \n",
    "    # even better \n",
    "    # the log of softmax is optimal to use in nn\n",
    "    # it scales things nicely without the log(0) issue\n",
    "    # since logarithms change multiplications to additions\n",
    "    # https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability\n",
    "    # https://medium.com/@AbhiramiVS/softmax-vs-logsoftmax-eb94254445a2\n",
    "    # log(e^zi/sum_j(e^zj) = log(e^zi) - sum_j(e^zj) = zi - sum_j(e^zj)\n",
    "    def logsoftmax(self, z):\n",
    "        # exponentiate\n",
    "        ze = tnp.exp(z)\n",
    "        # sum and log and un-squeeze\n",
    "        zes = tnp.sum(ze, axis=1, keepdims=True)\n",
    "        zesl = tnp.log(zes)\n",
    "        return (z - zesl)\n",
    "    \n",
    "    # cross entropy loss function\n",
    "    def __loss(self, Yhat, Y):\n",
    "        L = (Y*tnp.log(Yhat))+(1-Y)*tnp.log(1-Yhat)\n",
    "        if self.test_run:\n",
    "            print(f'L[0] -> {L[0]} ', end='')\n",
    "        return -L/self.m\n",
    "    \n",
    "    # objective function \n",
    "    # optimizing means we want to minimise this\n",
    "    # cost is the average of the loss over all training examples\n",
    "    # we are not actually going to need to compute this\n",
    "    # its just to understand the gradient partials\n",
    "    def __get_cost(self, a, y):\n",
    "        m = self.m\n",
    "        return tnp.sum(-y*tnp.log(a))/m\n",
    "        \n",
    "\n",
    "    # shuffle data\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def shuffle_data(self, X=None, Y=None, split=False, use_stored=False, overwrite=False, pass_back=True):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: print('no X data passed, to use stored data pass use_stored flag as True')\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: print('no Y data passed, to use stored data pass use_stored flag as True')\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        m = X.shape[0] \n",
    "        shuffle_index = tnp.random.permutation(m)\n",
    "        X, Y = X[shuffle_index, :], Y[shuffle_index, :]\n",
    "        if split:\n",
    "            split_data = self.split(X=X, Y=Y, overwrite=overwrite)\n",
    "            if overwrite:\n",
    "                self.split_data = split_data\n",
    "        if overwrite:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "        else: print('warning: overwrite not set, passing data back without storing.')\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return X,Y\n",
    "\n",
    "    # split data into 2 parts, give 1/train_ratio to test_X and test_y\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def split(self, X=None, Y=None, train_ratio=10, use_stored=False, overwrite=False, shuffle=False, pass_back=False):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: \n",
    "                print('no X data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: \n",
    "                print('no Y data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        train_X = X[:-(X.shape[0]//train_ratio)]\n",
    "        test_X = X[-(X.shape[0]//train_ratio):]\n",
    "        train_y = Y[:-(X.shape[0]//train_ratio)]\n",
    "        test_y = Y[-(X.shape[0]//train_ratio):]\n",
    "        split_data = (train_X, train_y, test_X, test_y)\n",
    "        if overwrite:\n",
    "            splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "            self.split_data = {a:b for a,b in zip(splits, split_data)}\n",
    "            self.m = self.split_data['tx'].shape[0]\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return split_data\n",
    "    \n",
    "    def batch_data(self, train_x, test_x, train_y, test_y, batch_size, shufffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        train_x_batches = [train_x[i:i+batch_size] for i in range(0, train_x.shape[0], batch_size)]\n",
    "        train_y_batches = [train_y[i:i+batch_size] for i in range(0, train_y.shape[0], batch_size)]\n",
    "        test_x_batches = [test_x[i:i+batch_size] for i in range(0, test_x.shape[0], batch_size)]\n",
    "        test_y_batches = [test_y[i:i+batch_size] for i in range(0, test_y.shape[0], batch_size)]\n",
    "        return (train_x_batches, train_y_batches, test_x_batches, test_y_batches)\n",
    "        \n",
    "    def set_data(self, X=None, Y=None, split:tuple=None, overwrite=False):\n",
    "        print('warning: this function has a lot of edge cases that arent covered\\n\\\n",
    "                be careful what you pass it...')\n",
    "        if all([X is None,Y is None,split is None]):\n",
    "            print('error: no data passed to set, please give data.')\n",
    "            return -1\n",
    "        elif split is None:\n",
    "            if (self.X is not None) or (self.Y is not None):\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                    return -1\n",
    "                else:\n",
    "                    self.X = X\n",
    "                    self.Y = Y\n",
    "                    if self.split_data is not None:\n",
    "                        self.foreign_split = True\n",
    "        elif all([X is None,Y is None]):\n",
    "            if self.split_data is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                else:\n",
    "                    if (split is not tuple):\n",
    "                        print('data error: split is not tuple be careful this part is sketchy')\n",
    "                    elif (len(split) != 4):\n",
    "                        print('data error: split should have 4 parts, be careful this part is sketchy')\n",
    "                    splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "                    self.split_data = {a:b for a,b in zip(splits, split)}\n",
    "                    self.m = self.split_data['tx'].shape[0]\n",
    "        else:\n",
    "            if not overwrite:\n",
    "                print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "            else:\n",
    "                if X is not None: self.X = X\n",
    "                if Y is not None: self.Y = Y\n",
    "                if split is not None: self.split_data = split\n",
    "    \n",
    "    def plot_image(self, x, y, yhat):\n",
    "        fig,ax = plt.subplots(figsize=(3,3),dpi=200)\n",
    "        ax.imshow(x.reshape(28,28), cmap=plt.get_cmap('nipy_spectral'))\n",
    "        ax.set_title(f'y:{tnp.argmax(y)}, yhat: {tnp.argmax(yhat)}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    def __forwardpass(self, X):\n",
    "        params = self.params\n",
    "        params['A0'] = X\n",
    "        if self.test_run:\n",
    "            print(f\"A0.shape = {params['A0'].shape}\")\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['Z'+str(i)] = tnp.matmul(params['A'+str(i-1)], params['W'+str(i)],) + params['B'+str(i)]\n",
    "            params['A'+str(i)] = self.sigmoid(params['Z'+str(i)])\n",
    "        params['Z'+str(len(self.sizes)-1)] = tnp.matmul(params['A'+str(len(self.sizes)-2)], params['W'+str(len(self.sizes)-1)],) +  params['B'+str(len(self.sizes)-1)]\n",
    "        params['A'+str(len(self.sizes)-1)] = self.softmax(params['Z'+str(len(self.sizes)-1)])\n",
    "        return params['A'+str(len(self.sizes)-1)]\n",
    "    \n",
    "    # gradient from chain rule on the derivative of the cost function\n",
    "    # cost is computed over a which is a function of z w and b\n",
    "    def __backprop(self, Y):\n",
    "        params = self.params\n",
    "        m = self.m\n",
    "        if self.test_run:\n",
    "            print(f'Y : {tnp.round(Y,3)}')\n",
    "            print(f\"A2 : {tnp.round(params['A2'], 3)}\")\n",
    "            print(f'shape Y : {Y.shape}')\n",
    "            print(f\"shape A2 : {params['A2'].shape}\")\n",
    "        \n",
    "        params['dZ'+str(len(self.sizes)-1)] = params['A'+str(len(self.sizes)-1)] - Y\n",
    "        params['dW'+str(len(self.sizes)-1)] = (1./m) * tnp.matmul(tnp.transpose(params['A'+str(len(self.sizes)-2)]), params['dZ'+str(len(self.sizes)-1)], )\n",
    "        params['dB'+str(len(self.sizes)-1)] = (1./m) * tnp.sum(params['dZ'+str(len(self.sizes)-1)], axis=0, keepdims=True)\n",
    "        for i in range(len(self.sizes)-2, 0, -1):\n",
    "            params['dA'+str(i)] = tnp.matmul(params['dZ'+str(i+1)], tnp.transpose(params['W'+str(i+1)]),)\n",
    "            params['dZ'+str(i)] = params['dA'+str(i)] * self.sigmoid_prime(params['Z'+str(i)])\n",
    "            params['dW'+str(i)] = (1./m) * tnp.matmul(tnp.transpose(params['A'+str(i-1)],), params['dZ'+str(i)],)\n",
    "            params['dB'+str(i)] = (1./m) * tnp.sum(params['dZ'+str(i)], axis=0, keepdims=True)\n",
    "        return \n",
    "    \n",
    "    def __update_network(self):\n",
    "        params = self.params\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['W'+str(i)] -= self.lr * params['dW'+str(i)]\n",
    "            params['B'+str(i)] -= self.lr * params['dB'+str(i)]\n",
    "        return\n",
    "        \n",
    "    def train_network(self, test_run=True, num_test_runs=1, train_X=None, train_y=None, output_times=True, batch_size=None):\n",
    "        times = self.times\n",
    "        self.test_run = test_run\n",
    "        params = self.params\n",
    "        self.J = 0\n",
    "        params['J'] = []\n",
    "        num_batches = 1\n",
    "        print_every = None\n",
    "        \n",
    "        # quick check for data and warning about loose code\n",
    "        if self.split_data is None:\n",
    "            print('\\n\\ndata error: Data not split, splitting now...')\n",
    "            if (self.X is None) or (self.Y is None):\n",
    "                print('\\ndata error: No Data, please give data with ann.set_data function or rebuid network.')\n",
    "                return -1\n",
    "            else:\n",
    "                self.split(use_stored=True, train_ratio=7, overwrite=True, shuffle=False, pass_back=False)\n",
    "                if batch_size != None:\n",
    "                    self.batch_size = batch_size\n",
    "                    print(f'\\nm = {self.m}\\n')\n",
    "                    num_batches = self.m // self.batch_size\n",
    "                    self.batch_d = self.batch_data(self.split_data['tx'], self.split_data['Tx'], self.split_data['ty'], self.split_data['Ty'], self.batch_size)\n",
    "                    print_every = num_batches\n",
    "        for i in range(self.epochs+1):\n",
    "            if self.batch_size != None:\n",
    "                if i % print_every == 0:\n",
    "                    start_time = time.time()\n",
    "                if test_run:\n",
    "                    print('\\nwarning: test run started. To train in full set test_run flag to False.')\n",
    "                if train_X is None:\n",
    "                    train_X = self.split_data['tx']\n",
    "                if train_y is None:\n",
    "                    train_y = self.split_data['ty']\n",
    "                if test_run:\n",
    "                    print(f'\\ntrain_X type : {type(train_X)}\\ntrain_X shape : {train_X.shape}')\n",
    "                    print(f'\\ntrain_y type : {type(train_X)}\\ntrain_y shape : {train_y.shape}')\n",
    "                    print(f'\\ntest_X type : {type(train_X)}\\ntest_X shape : {test_X.shape}')\n",
    "                    print(f'\\ntest_y type : {type(train_X)}\\ntest_y shape : {test_y.shape}')\n",
    "                if (i % batch_size == 0) and (i != 0):\n",
    "                    print('|:-', end='')\n",
    "                elif (i % batch_size != 0):\n",
    "                    print(f'{self.epochs-i}', end='')\n",
    "                A2 = self.__forwardpass(train_X)\n",
    "                if test_run:\n",
    "                    print(f'\\n\\nNETWORK TEST RUN PARAMS')\n",
    "                    print(f'After Forward Pass\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            else:\n",
    "                A2 = self.__forwardpass(train_X)\n",
    "                if i == 0:\n",
    "                    print('|  epoch  |  ERR:NOBATCHEE  %    |\\n|:-', end='')\n",
    "                if (i != num_batches) and (i % batch_size  != 0):\n",
    "                    print('-', end='')\n",
    "\n",
    "                if test_run:\n",
    "                    print(f'\\n\\nNETWORK TEST RUN PARAMS')\n",
    "                    print(f'After Forward Pass\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            \n",
    "                \n",
    "            J = self.__get_cost(A2, train_y)\n",
    "            params['J'] += [J]\n",
    "            \n",
    "            self.__backprop(train_y)\n",
    "            \n",
    "            if test_run:\n",
    "                if i == 1:\n",
    "                    print(f'After Backwards Propagation\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            self.__update_network()\n",
    "            # print(f'print_every : {print_every}')\n",
    "            if i == 0:\n",
    "                print(f'||::--------Progress--------::||\\n||::-{self.epochs}-', end='')\n",
    "           \n",
    "            elif ((i % print_every == print_every-1) or test_run):\n",
    "                stop_time = time.time()\n",
    "                epoch_time = np.round(((stop_time - start_time)/print_every), 2)\n",
    "                times += [epoch_time]\n",
    "                print(\"-::||  epoch \", i, \" cost: \", np.round(tf.cast(J, dtype=tf.float64), 3) , end=' ')\n",
    "                print(f' {epoch_time} sec per epoch ', end='')\n",
    "                if i % batch_size*num_batches//4 == 0:\n",
    "                    self.test_network()\n",
    "                else:\n",
    "                    print()\n",
    "                \n",
    "                print('||::-', end='')\n",
    "                # if (i * batch size // num_batches != 0):\n",
    "                #     print('|:- ', end='')\n",
    "            \n",
    "            else:\n",
    "                print('-', end='')\n",
    "                \n",
    "                    \n",
    "            if test_run:\n",
    "                if i>= num_test_runs:\n",
    "                    return\n",
    "        self.test_network()\n",
    "        \n",
    "    def test_network(self, test_X=None, test_y=None):\n",
    "        if test_X is None:\n",
    "            test_X = self.split_data['Tx']\n",
    "        if test_y is None:\n",
    "            test_y = self.split_data['Ty']\n",
    "        \n",
    "        A2 = self.__forwardpass(test_X)\n",
    "        score = 0\n",
    "        for yhat,y in zip(A2,test_y):\n",
    "            if tnp.argmax(yhat) == tnp.argmax(y):\n",
    "                score+=1\n",
    "        accuracy = score/len(test_y)\n",
    "        print(f'val acc = {np.round(accuracy,2)*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53767b66-2d38-46cf-9996-e9c87534a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: del(nn)\n",
    "except NameError: print('it wasnt a thing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e922678-239f-4875-b731-e70643b90633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "data error: Data not split, splitting now...\n",
      "\n",
      "m = 60000\n",
      "\n",
      "||::--------Progress--------::||\n",
      "||::-100-99-98-97-96-95-::||  epoch  5  cost:  2.637  1.0 sec / epoch \n",
      "||::-94-93-92-91-90-89-::||  epoch  11  cost:  1.323  0.98 sec / epoch \n",
      "||::-88-87-86-85-84-83-::||  epoch  17  cost:  0.753  0.97 sec / epoch \n",
      "||::-82-81-80-79-78-77-::||  epoch  23  cost:  0.586  0.97 sec / epoch \n",
      "||::-76-75-74-73-72-71-::||  epoch  29  cost:  0.518  0.96 sec / epoch \n",
      "||::-70-69-68-67-66-65-::||  epoch  35  cost:  0.472  1.01 sec / epoch \n",
      "||::-64-63-62-61-60-59-::||  epoch  41  cost:  0.438  1.15 sec / epoch \n",
      "||::-58-57-56-55-54-53-::||  epoch  47  cost:  0.411  1.05 sec / epoch \n",
      "||::-52-51-50-49-48-47-::||  epoch  53  cost:  0.388  1.09 sec / epoch \n",
      "||::-46-45-44-43-42-41-::||  epoch  59  cost:  0.37  1.07 sec / epoch \n",
      "||::-40-39-38-37-36-35-::||  epoch  65  cost:  0.354  1.38 sec / epoch \n",
      "||::-34-33-32-31-30-29-::||  epoch  71  cost:  0.341  1.07 sec / epoch \n",
      "||::-28-27-26-25-24-23-::||  epoch  77  cost:  0.329  1.26 sec / epoch \n",
      "||::-22-21-20-19-18-17-::||  epoch  83  cost:  0.319  1.23 sec / epoch \n",
      "||::-16-15-14-13-12-11-::||  epoch  89  cost:  0.31  0.98 sec / epoch \n",
      "||::-10-9-8-7-6-5-::||  epoch  95  cost:  0.302  1.0 sec / epoch \n",
      "||::-4-3-2-1-0-val acc = 92.0\n"
     ]
    }
   ],
   "source": [
    "nn = ann(sizes=[784, 128, 10], rate=10, epochs=100, X=X, Y=Y)\n",
    "nn.split(use_stored=True, pass_back=False)\n",
    "nn.train_network(test_run=False, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc = 92.0\n"
     ]
    }
   ],
   "source": [
    "nn.test_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2",
   "language": "python",
   "name": "data2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
