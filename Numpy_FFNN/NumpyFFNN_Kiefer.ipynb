{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c6f116-9d4f-4178-a9f9-8a760ac15f25",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Neural Network From Scratch Attempt #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07da492c-5c8a-4ee5-900a-fe2588db7def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffba520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0z/9nrqpdg94f9976b246v2q0jm0000gn/T/ipykernel_8155/2929329053.py:5: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n"
     ]
    }
   ],
   "source": [
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "# Usage example:\n",
    "train_X = read_idx('../Data/mnist/train-images-idx3-ubyte')\n",
    "train_y = read_idx('../Data/mnist/train-labels-idx1-ubyte')\n",
    "test_X = read_idx('../Data/mnist/t10k-images-idx3-ubyte')\n",
    "test_y = read_idx('../Data/mnist/t10k-labels-idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30f15b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8362167-5189-44e2-9def-3277bc4e46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "num_pixels = train_X.shape[1]*train_X.shape[2]\n",
    "# flatten X\n",
    "train_X = train_X.reshape(train_X.shape[0], num_pixels).astype('float32')\n",
    "test_X = test_X.reshape(test_X.shape[0], num_pixels).astype('float32')\n",
    "# one hot categorical Y\n",
    "train_hot = np.eye(np.max(train_y)+1)[train_y].astype('float32')\n",
    "test_hot = np.eye(np.max(test_y)+1)[test_y].astype('float32')\n",
    "# lets put these back together so that we can actually shuffle them later (normalize too)\n",
    "X = np.concatenate((train_X, test_X), axis=0)/255\n",
    "Y = np.concatenate((train_hot, test_hot), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e97820b3-69e1-4122-baef-a85c9a099ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = 70000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = X.shape[0]\n",
    "print(f'm = {m}')\n",
    "n = 5\n",
    "fig,axs = plt.subplots(1, n, figsize=(10,2),dpi=100)\n",
    "for i in range(n):\n",
    "    rnd = np.random.randint(m)\n",
    "    # plotting first few images from the dataset\n",
    "    _=axs[i].imshow(X[rnd].reshape(28,28), cmap=plt.get_cmap('nipy_spectral'))\n",
    "    _=axs[i].set_title(f'{np.argmax(Y[rnd])}')\n",
    "    _=axs[i].axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa68e0c-bcf9-4f88-9436-bd19c5b2c646",
   "metadata": {},
   "source": [
    "<center><u>components of the gradient</u></center>\n",
    "<br>\n",
    "<center>regression predicts variables $w, b$</center>\n",
    "\n",
    "<center>$x, y$ are constants</center>\n",
    "<br>\n",
    "<center><u>logistic regression process</u></center>\n",
    "\n",
    "$$Z = W^TX + B$$\n",
    "\n",
    "$$A = \\sigma(Z)$$\n",
    "\n",
    "$$A_{outputs} = \\hat{Y}$$\n",
    "\n",
    "$$L = L(\\hat{Y}, Y) = L(A_{out}, y)$$\n",
    "\n",
    "$$L\\ = - y \\log{(A_{out})}\\ +\\ (1 - y)\\log{(1 - A_{out})}$$\n",
    "\n",
    "<center>so we need the output of the forward pass i.e.  \n",
    "\n",
    "$$\\hat{y}$$\n",
    "\n",
    "to compute the derivative of the loss function</center>\n",
    "\n",
    "<center><u>Chain Rule</u></center>\n",
    "\n",
    "$$Z \\rightarrow A \\rightarrow L$$\n",
    "\n",
    "$$z = w_1x_1 + w_2x_2 + \\ldots + b$$\n",
    "\n",
    "$$dw_1 =  \\frac{dL}{dw_1} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{w_1}}$$\n",
    "\n",
    "$$ = x_1 \\cdot \\frac{\\partial{z}}{\\partial{w_1}}$$\n",
    "\n",
    "$$dw_2 =  \\frac{dL}{dw_2} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{w_2}}$$\n",
    "\n",
    "$$ = x_2 \\cdot \\frac{\\partial{z}}{\\partial{w_2}}$$\n",
    "\n",
    "$$dw_b =  \\frac{dL}{db} = \\frac{\\partial{L}}{\\partial{z}} \\cdot \\frac{\\partial{z}}{\\partial{b}}$$\n",
    "\n",
    "$$ ? = \\frac{\\partial{z}}{\\partial{b}}$$\n",
    "\n",
    "***\n",
    "\n",
    "$$\\frac{\\partial{z}}{\\partial{w_1}} + \\frac{\\partial{z}}{\\partial{w_2}} + \\ldots \\frac{\\partial{z}}{\\partial{w_m}} + \\frac{\\partial{z}}{\\partial{b}} = dZ$$\n",
    "\n",
    "$$dz = \\frac{\\partial{L}}{\\partial{z}} = \\frac{\\partial{L}}{\\partial{a}} \\cdot \\frac{\\partial{a}}{\\partial{z}}$$\n",
    "\n",
    "$$ = \\bigl( - \\frac{y}{a} + \\frac{1-y}{1-a}\\bigr) \\cdot a(1-a)$$\n",
    "\n",
    "$$ = a - y$$\n",
    "\n",
    "***\n",
    "\n",
    "$$da = \\frac{\\partial{L}}{\\partial{a}} = - \\frac{y}{a} + \\frac{1-y}{1-a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07c680fa-0c74-427b-843f-b78a2ca5ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann():\n",
    "    # sizes are a list describing the dense layers\n",
    "    # the itnput size and output size for now will be inferred\n",
    "    # from the size of the X and Y data\n",
    "    # likely  could implement something where itnput and output layer are  not  produced  \n",
    "    # until data is offered, im not sure if this is a bad idea. ill have to research a bit.\n",
    "    # data examples are expected to be provided as rows\n",
    "    def __init__(self, sizes=None, epochs=1, rate=0.001, X=None, Y=None):\n",
    "        self.sizes = None\n",
    "        self.num_layers = None\n",
    "        self.m = None\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.J = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.batch_size=0\n",
    "        self.times = []\n",
    "        self.test_run = False\n",
    "        self.split_data = None\n",
    "        self.foreign_split = False\n",
    "        self.epochs = epochs\n",
    "        self.lr = rate\n",
    "        self.params = self.build_init(sizes)\n",
    "    \n",
    "    def build_init(self, sizes:list=None, acv_hidden:object=None, acv_output:object=None, overwrite=False):\n",
    "        if self.sizes is None:\n",
    "            if sizes is None:\n",
    "                print('error: no sizes to create layer params, please give sizes.')\n",
    "                return\n",
    "        else:\n",
    "            if sizes is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite build without overwrite flag,\\n\\\n",
    "                                please  set flag if you want to rebuild.')\n",
    "                    return\n",
    "                else:\n",
    "                    self.sizes = sizes\n",
    "                    params = self.params # not really sure what i am doing here\n",
    "        self.sizes = sizes\n",
    "        params = self.params = {}\n",
    "        for i in range(1,len(self.sizes)):\n",
    "            params['W'+str(i)] = tnp.random.uniform(-1,1,(self.sizes[i-1], self.sizes[i]))\n",
    "            params['dW'+str(i)] = tnp.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "            params['B'+str(i)] = tnp.zeros((1,self.sizes[i]))\n",
    "            params['dB'+str(i)] = tnp.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "        return params\n",
    "    \n",
    "    # activation funtion for hidden layer\n",
    "    # logistic function\n",
    "    def sigmoid(self, z):\n",
    "        A = 1./(1+tnp.exp(-z))\n",
    "        return A\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return (self.sigmoid(z) * (1 - self.sigmoid(z)))\n",
    "        \n",
    "    # activation function for output layer\n",
    "    # softmax is the probability function density with exponentiation\n",
    "    def softmax(self, z):\n",
    "        # normalize to stabilize for large exponentials\n",
    "        zn =  z - tf.reduce_max(z, axis=-1, keepdims=True)\n",
    "        # exponentiate\n",
    "        zne = tnp.exp(zn)\n",
    "        # sum\n",
    "        znes = tnp.sum(zne, axis=-1, keepdims=True)\n",
    "        # learned to add this constant in  csc421\n",
    "        return zne / znes  + 1e-15\n",
    "    \n",
    "    # even better \n",
    "    # the log of softmax is optimal to use in nn\n",
    "    # it scales things nicely without the log(0) issue\n",
    "    # since logarithms change multiplications to additions\n",
    "    # https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability\n",
    "    # https://medium.com/@AbhiramiVS/softmax-vs-logsoftmax-eb94254445a2\n",
    "    # log(e^zi/sum_j(e^zj) = log(e^zi) - sum_j(e^zj) = zi - sum_j(e^zj)\n",
    "    def logsoftmax(self, z):\n",
    "        # exponentiate\n",
    "        ze = tnp.exp(z)\n",
    "        # sum and log and un-squeeze\n",
    "        zes = tnp.sum(ze, axis=1, keepdims=True)\n",
    "        zesl = tnp.log(zes)\n",
    "        return (z - zesl)\n",
    "    \n",
    "    # cross entropy loss function\n",
    "    def __loss(self, Yhat, Y):\n",
    "        L = (Y*tnp.log(Yhat))+(1-Y)*tnp.log(1-Yhat)\n",
    "        if self.test_run:\n",
    "            print(f'L[0] -> {L[0]} ', end='')\n",
    "        return -L/self.m\n",
    "    \n",
    "    # objective function \n",
    "    # optimizing means we want to minimise this\n",
    "    # cost is the average of the loss over all training examples\n",
    "    # we are not actually going to need to compute this\n",
    "    # its just to understand the gradient partials\n",
    "    def __get_cost(self, a, y):\n",
    "        m = self.m\n",
    "        return tnp.sum(-y*tnp.log(a))/m\n",
    "        \n",
    "\n",
    "    # shuffle data\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def shuffle_data(self, X=None, Y=None, split=False, use_stored=False, overwrite=False, pass_back=True):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: print('no X data passed, to use stored data pass use_stored flag as True')\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: print('no Y data passed, to use stored data pass use_stored flag as True')\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        m = X.shape[0] \n",
    "        shuffle_index = tnp.random.permutation(m)\n",
    "        X, Y = X[shuffle_index, :], Y[shuffle_index, :]\n",
    "        if split:\n",
    "            split_data = self.split(X=X, Y=Y, overwrite=overwrite)\n",
    "            if overwrite:\n",
    "                self.split_data = split_data\n",
    "        if overwrite:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "        else: print('warning: overwrite not set, passing data back without storing.')\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return X,Y\n",
    "\n",
    "    # split data into 2 parts, give 1/train_ratio to test_X and test_y\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def split(self, X=None, Y=None, train_ratio=10, use_stored=False, overwrite=False, shuffle=False, pass_back=False):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: \n",
    "                print('no X data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: \n",
    "                print('no Y data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        train_X = X[:-(X.shape[0]//train_ratio)]\n",
    "        test_X = X[-(X.shape[0]//train_ratio):]\n",
    "        train_y = Y[:-(X.shape[0]//train_ratio)]\n",
    "        test_y = Y[-(X.shape[0]//train_ratio):]\n",
    "        split_data = (train_X, train_y, test_X, test_y)\n",
    "        if overwrite:\n",
    "            splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "            self.split_data = {a:b for a,b in zip(splits, split_data)}\n",
    "            self.m = self.split_data['tx'].shape[0]\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return split_data\n",
    "    \n",
    "    def batch_dats(self, train_x, test_x, train_y, test_y, batch_size, shufffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        train_x_batches = [train_x[i:i+batch_size] for i in range(0, train_x.shape[0], batch_size)]\n",
    "        train_y_batches = [train_y[i:i+batch_size] for i in range(0, train_y.shape[0], batch_size)]\n",
    "        test_x_batches = [test_x[i:i+batch_size] for i in range(0, test_x.shape[0], batch_size)]\n",
    "        test_y_batches = [test_y[i:i+batch_size] for i in range(0, test_y.shape[0], batch_size)]\n",
    "        return train_x_batches, train_y_batches, test_x_batches, test_y_batches\n",
    "        \n",
    "    def set_data(self, X=None, Y=None, split:tuple=None, overwrite=False):\n",
    "        print('warning: this function has a lot of edge cases that arent covered\\n\\\n",
    "                be careful what you pass it...')\n",
    "        if all([X is None,Y is None,split is None]):\n",
    "            print('error: no data passed to set, please give data.')\n",
    "            return -1\n",
    "        elif split is None:\n",
    "            if (self.X is not None) or (self.Y is not None):\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                    return -1\n",
    "                else:\n",
    "                    self.X = X\n",
    "                    self.Y = Y\n",
    "                    if self.split_data is not None:\n",
    "                        self.foreign_split = True\n",
    "        elif all([X is None,Y is None]):\n",
    "            if self.split_data is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                else:\n",
    "                    if (split is not tuple):\n",
    "                        print('data error: split is not tuple be careful this part is sketchy')\n",
    "                    elif (len(split) != 4):\n",
    "                        print('data error: split should have 4 parts, be careful this part is sketchy')\n",
    "                    splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "                    self.split_data = {a:b for a,b in zip(splits, split)}\n",
    "                    self.m = self.split_data['tx'].shape[0]\n",
    "        else:\n",
    "            if not overwrite:\n",
    "                print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "            else:\n",
    "                if X is not None: self.X = X\n",
    "                if Y is not None: self.Y = Y\n",
    "                if split is not None: self.split_data = split\n",
    "    \n",
    "    def plot_image(self, x, y, yhat):\n",
    "        fig,ax = plt.subplots(figsize=(3,3),dpi=200)\n",
    "        ax.imshow(x.reshape(28,28), cmap=plt.get_cmap('nipy_spectral'))\n",
    "        ax.set_title(f'y:{tnp.argmax(y)}, yhat: {tnp.argmax(yhat)}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    def __forwardpass(self, X):\n",
    "        params = self.params\n",
    "        params['A0'] = X\n",
    "        if self.test_run:\n",
    "            print(f\"A0.shape = {params['A0'].shape}\")\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['Z'+str(i)] = tnp.matmul(params['A'+str(i-1)], params['W'+str(i)],) + params['B'+str(i)]\n",
    "            params['A'+str(i)] = self.sigmoid(params['Z'+str(i)])\n",
    "        params['Z'+str(len(self.sizes)-1)] = tnp.matmul(params['A'+str(len(self.sizes)-2)], params['W'+str(len(self.sizes)-1)],) +  params['B'+str(len(self.sizes)-1)]\n",
    "        params['A'+str(len(self.sizes)-1)] = self.softmax(params['Z'+str(len(self.sizes)-1)])\n",
    "        return params['A'+str(len(self.sizes)-1)]\n",
    "    \n",
    "    # gradient from chain rule on the derivative of the cost function\n",
    "    # cost is computed over a which is a function of z w and b\n",
    "    def __backprop(self, Y):\n",
    "        params = self.params\n",
    "        m = self.m\n",
    "        if self.test_run:\n",
    "            print(f'Y : {tnp.round(Y,3)}')\n",
    "            print(f\"A2 : {tnp.round(params['A2'], 3)}\")\n",
    "            print(f'shape Y : {Y.shape}')\n",
    "            print(f\"shape A2 : {params['A2'].shape}\")\n",
    "        \n",
    "        params['dZ'+str(len(self.sizes)-1)] = params['A'+str(len(self.sizes)-1)] - Y\n",
    "        params['dW'+str(len(self.sizes)-1)] = (1./m) * tnp.matmul(tnp.transpose(params['A'+str(len(self.sizes)-2)]), params['dZ'+str(len(self.sizes)-1)], )\n",
    "        params['dB'+str(len(self.sizes)-1)] = (1./m) * tnp.sum(params['dZ'+str(len(self.sizes)-1)], axis=0, keepdims=True)\n",
    "        for i in range(len(self.sizes)-2, 0, -1):\n",
    "            params['dA'+str(i)] = tnp.matmul(params['dZ'+str(i+1)], tnp.transpose(params['W'+str(i+1)]),)\n",
    "            params['dZ'+str(i)] = params['dA'+str(i)] * self.sigmoid_prime(params['Z'+str(i)])\n",
    "            params['dW'+str(i)] = (1./m) * tnp.matmul(tnp.transpose(params['A'+str(i-1)],), params['dZ'+str(i)],)\n",
    "            params['dB'+str(i)] = (1./m) * tnp.sum(params['dZ'+str(i)], axis=0, keepdims=True)\n",
    "        return \n",
    "    \n",
    "    def __update_network(self):\n",
    "        params = self.params\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['W'+str(i)] -= self.lr * params['dW'+str(i)]\n",
    "            params['B'+str(i)] -= self.lr * params['dB'+str(i)]\n",
    "        return\n",
    "        \n",
    "    def train_network(self, test_run=True, num_test_runs=1, train_X=None, train_y=None, output_times=True, batch_size=10):\n",
    "        times = self.times\n",
    "        num_batches=1;\n",
    "        print_every = 10\n",
    "        self.test_run = test_run\n",
    "        params = self.params\n",
    "        self.J = 0\n",
    "        params['J'] = []\n",
    "        if batch_size !=0:\n",
    "            self.batch_size = batch_size\n",
    "            num_batches = self.m // self.batch_size\n",
    "        # quick check for data and warning about loose code\n",
    "        if self.split_data is None:\n",
    "            print('\\n\\ndata error: Data not split, splitting now...')\n",
    "            if (self.X is None) or (self.Y is None):\n",
    "                print('\\ndata error: No Data, please give data with ann.set_data function or rebuid network.')\n",
    "                return -1\n",
    "            else: \n",
    "                self.split(use_stored=True, train_ratio=7, overwrite=True, shuffle=False, pass_back=False)\n",
    "        \n",
    "        for i in range(self.epochs+1):\n",
    "            if self.batch_size != 0:\n",
    "                if i % print_every == 1:\n",
    "                    start_time = time.time()\n",
    "                if test_run:\n",
    "                    print('\\nwarning: test run started. To train in full set test_run flag to False.')\n",
    "                if train_X is None:\n",
    "                    train_X = self.split_data['tx']\n",
    "                if train_y is None:\n",
    "                    train_y = self.split_data['ty']\n",
    "                if test_run:\n",
    "                    print(f'\\ntrain_X type : {type(train_X)}\\ntrain_X shape : {train_X.shape}')\n",
    "                    print(f'\\ntrain_y type : {type(train_X)}\\ntrain_y shape : {train_y.shape}')\n",
    "                    print(f'\\ntest_X type : {type(train_X)}\\ntest_X shape : {test_X.shape}')\n",
    "                    print(f'\\ntest_y type : {type(train_X)}\\ntest_y shape : {test_y.shape}')\n",
    "            \n",
    "                A2 = self.__forwardpass(train_X)\n",
    "                if test_run:\n",
    "                    print(f'\\n\\nNETWORK TEST RUN PARAMS')\n",
    "                    print(f'After Forward Pass\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            else:\n",
    "                A2 = self.__forwardpass(train_X)\n",
    "                if i == 0:\n",
    "                    print('|  epoch  |    %    |\\n|| ', end='')\n",
    "                if (i != num_batches) and (i % print_every  != 0):\n",
    "                    print(' >', end='')\n",
    "\n",
    "                if test_run:\n",
    "                    print(f'\\n\\nNETWORK TEST RUN PARAMS')\n",
    "                    print(f'After Forward Pass\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            \n",
    "                \n",
    "            J = self.__get_cost(A2, train_y)\n",
    "            params['J'] += [J]\n",
    "            \n",
    "            self.__backprop(train_y)\n",
    "            \n",
    "            if test_run:\n",
    "                if i == 1:\n",
    "                    print(f'After Backwards Propagation\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            self.__update_network()\n",
    "            # if i == 0:\n",
    "                # print(\"Epoch\", i, \"cost: \", np.round(tf.cast(J, dtype=tf.float64), 3))\n",
    "            if ((i != 0) and (i % self.m//num_batches == 0)) or test_run:\n",
    "                stop_time = time.time()\n",
    "                epoch_time = np.round((stop_time - start_time)/10,2)\n",
    "                times += [epoch_time]\n",
    "                print(\" |:: epoch \", i, \" cost: \", np.round(tf.cast(J, dtype=tf.float64), 3) , end=' :')\n",
    "                print(f':  -- :{epoch_time} sec/epoch')\n",
    "                if (i != num_batches) and (i % print_every  == 0):\n",
    "                    print('|| ', end='')\n",
    "            if test_run:\n",
    "                if i>= num_test_runs:\n",
    "                    return\n",
    "        \n",
    "    def test_network(self, test_X=None, test_y=None):\n",
    "        if test_X is None:\n",
    "            test_X = self.split_data['Tx']\n",
    "        if test_y is None:\n",
    "            test_y = self.split_data['Ty']\n",
    "        A2 = self.__forwardpass(test_X)\n",
    "        score = 0\n",
    "        for yhat,y in zip(A2,test_y):\n",
    "            if np.argmax(yhat) == tnp.argmax(y):\n",
    "                score+=1\n",
    "        accuracy = score/len(test_y)\n",
    "        print(f'test acc = {np.round(accuracy,2)*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53767b66-2d38-46cf-9996-e9c87534a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it wasnt a thing.\n"
     ]
    }
   ],
   "source": [
    "try: del(nn)\n",
    "except NameError: print('it wasnt a thing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3e922678-239f-4875-b731-e70643b90633",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [69], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m nn \u001b[39m=\u001b[39m ann(sizes\u001b[39m=\u001b[39m[\u001b[39m784\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m10\u001b[39m], rate\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, epochs\u001b[39m=\u001b[39m\u001b[39m150\u001b[39m, X\u001b[39m=\u001b[39mX, Y\u001b[39m=\u001b[39mY)\n\u001b[1;32m      2\u001b[0m nn\u001b[39m.\u001b[39msplit(use_stored\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, pass_back\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m nn\u001b[39m.\u001b[39;49mtrain_network(test_run\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn [67], line 282\u001b[0m, in \u001b[0;36mann.train_network\u001b[0;34m(self, test_run, num_test_runs, train_X, train_y, output_times, batch_size)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m batch_size \u001b[39m!=\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m    281\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size \u001b[39m=\u001b[39m batch_size\n\u001b[0;32m--> 282\u001b[0m     num_batches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size\n\u001b[1;32m    283\u001b[0m \u001b[39m# quick check for data and warning about loose code\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for //: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "nn = ann(sizes=[784, 128, 10], rate=10, epochs=150, X=X, Y=Y)\n",
    "nn.split(use_stored=True, pass_back=False)\n",
    "nn.train_network(test_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3dc569-651d-40f1-abc6-05feee1b1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc = 93.0\n"
     ]
    }
   ],
   "source": [
    "nn.test_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c680fa-0c74-427b-843f-b78a2ca5ea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ann_np():\n",
    "    # sizes are a list describing the dense layers\n",
    "    # the itnput size and output size for now will be inferred\n",
    "    # from the size of the X and Y data\n",
    "    # likely  could implement something where itnput and output layer are  not  produced  \n",
    "    # until data is offered, im not sure if this is a bad idea. ill have to research a bit.\n",
    "    # data examples are expected to be provided as rows\n",
    "    def __init__(self, sizes=None, epochs=1, rate=0.001, X=None, Y=None):\n",
    "        self.sizes = None\n",
    "        self.num_layers = None\n",
    "        self.m = None\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.J = None\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        self.times = []\n",
    "        self.test_run = False\n",
    "        self.split_data = None\n",
    "        self.foreign_split = False\n",
    "        self.epochs = epochs\n",
    "        self.lr = rate\n",
    "        self.params = self.build_init(sizes)\n",
    "    \n",
    "    def build_init(self, sizes:list=None, acv_hidden:object=None, acv_output:object=None, overwrite=False):\n",
    "        if self.sizes is None:\n",
    "            if sizes is None:\n",
    "                print('error: no sizes to create layer params, please give sizes.')\n",
    "                return\n",
    "        else:\n",
    "            if sizes is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite build without overwrite flag,\\n\\\n",
    "                                please  set flag if you want to rebuild.')\n",
    "                    return\n",
    "                else:\n",
    "                    self.sizes = sizes\n",
    "                    params = self.params # not really sure what i am doing here\n",
    "        self.sizes = sizes\n",
    "        params = self.params = {}\n",
    "        for i in range(1,len(self.sizes)):\n",
    "            params['W'+str(i)] = np.random.uniform(-1,1,(self.sizes[i-1], self.sizes[i]))\n",
    "            params['dW'+str(i)] = np.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "            params['B'+str(i)] = np.zeros((1,self.sizes[i]))\n",
    "            params['dB'+str(i)] = np.zeros((self.sizes[i-1], self.sizes[i]))\n",
    "        return params\n",
    "    \n",
    "    # activation funtion for hidden layer\n",
    "    # logistic function\n",
    "    def sigmoid(self, z):\n",
    "        A = 1./(1+np.exp(-z))\n",
    "        return A\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return (self.sigmoid(z) * (1 - self.sigmoid(z)))\n",
    "        \n",
    "    # activation function for output layer\n",
    "    # softmax is the probability function density with exponentiation\n",
    "    def softmax(self, z):\n",
    "        # normalize to stabilize for large exponentials\n",
    "        zn =  z - z.max(axis=-1, keepdims=True)\n",
    "        # exponentiate\n",
    "        zne = np.exp(zn)\n",
    "        # sum\n",
    "        znes = np.sum(zne, axis=-1, keepdims=True)\n",
    "        # learned to add this constant in  csc421\n",
    "        return zne / znes  + 1e-15\n",
    "    \n",
    "    # even better \n",
    "    # the log of softmax is optimal to use in nn\n",
    "    # it scales things nicely without the log(0) issue\n",
    "    # since logarithms change multiplications to additions\n",
    "    # https://stats.stackexchange.com/questions/174481/why-to-optimize-max-log-probability-instead-of-probability\n",
    "    # https://medium.com/@AbhiramiVS/softmax-vs-logsoftmax-eb94254445a2\n",
    "    # log(e^zi/sum_j(e^zj) = log(e^zi) - sum_j(e^zj) = zi - sum_j(e^zj)\n",
    "    def logsoftmax(self, z):\n",
    "        # exponentiate\n",
    "        ze = np.exp(z)\n",
    "        # sum and log and un-squeeze\n",
    "        zes = np.sum(ze, axis=1, keepdims=True)\n",
    "        zesl = np.log(zes)\n",
    "        return (z - zesl)\n",
    "    \n",
    "    # cross entropy loss function\n",
    "    def __loss(self, Yhat, Y):\n",
    "        L = (Y*np.log(Yhat))+(1-Y)*np.log(1-Yhat)\n",
    "        if self.test_run:\n",
    "            print(f'L[0] -> {L[0]} ', end='')\n",
    "        return -L/self.m\n",
    "    \n",
    "    # objective function \n",
    "    # optimizing means we want to minimise this\n",
    "    # cost is the average of the loss over all training examples\n",
    "    # we are not actually going to need to compute this\n",
    "    # its just to understand the gradient partials\n",
    "    def __get_cost(self, a, y):\n",
    "        m = self.m\n",
    "        return np.sum(-y*tnp.log(a))/m\n",
    "        \n",
    "\n",
    "    # shuffle data\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def shuffle_data(self, X=None, Y=None, split=False, use_stored=False, overwrite=False, pass_back=True):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: print('no X data passed, to use stored data pass use_stored flag as True')\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y)\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: print('no Y data passed, to use stored data pass use_stored flag as True')\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        m = X.shape[0] \n",
    "        shuffle_index = tnp.random.permutation(m)\n",
    "        X, Y = X[shuffle_index, :], Y[shuffle_index, :]\n",
    "        if split:\n",
    "            split_data = self.split(X=X, Y=Y, overwrite=overwrite)\n",
    "            if overwrite:\n",
    "                self.split_data = split_data\n",
    "        if overwrite:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "        else: print('warning: overwrite not set, passing data back without storing.')\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return X,Y\n",
    "\n",
    "    # split data into 2 parts, give 1/train_ratio to test_X and test_y\n",
    "    # samples are assumed to be stacked in rows\n",
    "    def split(self, X=None, Y=None, train_ratio=10, use_stored=False, overwrite=False, shuffle=False, pass_back=False):\n",
    "        if X is None:\n",
    "            if self.X is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                X = self.X\n",
    "            else: \n",
    "                print('no X data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if Y is None:\n",
    "            if self.Y is None:\n",
    "                print(\"data error: no X data provided, please pass data to this\\n\\\n",
    "                            function or assign network data with ann.set_data(X,Y).\")\n",
    "                return -1\n",
    "            elif use_stored:\n",
    "                Y = self.Y\n",
    "            else: \n",
    "                print('no Y data passed, to use stored data pass use_stored flag as True.')\n",
    "                return -1\n",
    "        if X.shape[0] != Y.shape[0]:\n",
    "            print('data error: X and Y data have different lengths\\n\\\n",
    "                        please pass network similar data')\n",
    "            return -1\n",
    "        train_X = X[:-(X.shape[0]//train_ratio)]\n",
    "        test_X = X[-(X.shape[0]//train_ratio):]\n",
    "        train_y = Y[:-(X.shape[0]//train_ratio)]\n",
    "        test_y = Y[-(X.shape[0]//train_ratio):]\n",
    "        split_data = (train_X, train_y, test_X, test_y)\n",
    "        if overwrite:\n",
    "            splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "            self.split_data = {a:b for a,b in zip(splits, split_data)}\n",
    "            self.m = self.split_data['tx'].shape[0]\n",
    "        if not pass_back:\n",
    "            return\n",
    "        else:\n",
    "            return split_data\n",
    "    \n",
    "    def set_data(self, X=None, Y=None, split:tuple=None, overwrite=False):\n",
    "        print('warning: this function has a lot of edge cases that arent covered\\n\\\n",
    "                be careful what you pass it...')\n",
    "        if all([X is None,Y is None,split is None]):\n",
    "            print('error: no data passed to set, please give data.')\n",
    "            return -1\n",
    "        elif split is None:\n",
    "            if (self.X is not None) or (self.Y is not None):\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                    return -1\n",
    "                else:\n",
    "                    self.X = X\n",
    "                    self.Y = Y\n",
    "                    if self.split_data is not None:\n",
    "                        self.foreign_split = True\n",
    "        elif all([X is None,Y is None]):\n",
    "            if self.split_data is not None:\n",
    "                if not overwrite:\n",
    "                    print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "                else:\n",
    "                    if (split is not tuple):\n",
    "                        print('data error: split is not tuple be careful this part is sketchy')\n",
    "                    elif (len(split) != 4):\n",
    "                        print('data error: split should have 4 parts, be careful this part is sketchy')\n",
    "                    splits = ['tx', 'ty', 'Tx', 'Ty']\n",
    "                    self.split_data = {a:b for a,b in zip(splits, split)}\n",
    "                    self.m = self.split_data['tx'].shape[0]\n",
    "        else:\n",
    "            if not overwrite:\n",
    "                print('error: cannot overwrite data with unset overwrite flag, please set the flag in this call to true.')\n",
    "            else:\n",
    "                if X is not None: self.X = X\n",
    "                if Y is not None: self.Y = Y\n",
    "                if split is not None: self.split_data = split\n",
    "    \n",
    "    def plot_image(self, x, y, yhat):\n",
    "        fig,ax = plt.subplots(figsize=(3,3),dpi=200)\n",
    "        ax.imshow(x.reshape(28,28), cmap=plt.get_cmap('nipy_spectral'))\n",
    "        ax.set_title(f'y:{tnp.argmax(y)}, yhat: {tnp.argmax(yhat)}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    def __forwardpass(self, X):\n",
    "        params = self.params\n",
    "        params['A0'] = X\n",
    "        if self.test_run:\n",
    "            print(f\"A0.shape = {params['A0'].shape}\")\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['Z'+str(i)] = params['A'+str(i-1)] @  params['W'+str(i)] + params['B'+str(i)]\n",
    "            params['A'+str(i)] = self.sigmoid(params['Z'+str(i)])\n",
    "        params['Z'+str(len(self.sizes)-1)] = params['A'+str(len(self.sizes)-2)] @ params['W'+str(len(self.sizes)-1)] +  params['B'+str(len(self.sizes)-1)]\n",
    "        params['A'+str(len(self.sizes)-1)] = self.softmax(params['Z'+str(len(self.sizes)-1)])\n",
    "        return params['A'+str(len(self.sizes)-1)]\n",
    "    \n",
    "    # gradient from chain rule on the derivative of the cost function\n",
    "    # cost is computed over a which is a function of z w and b\n",
    "    def __backprop(self, Y):\n",
    "        params = self.params\n",
    "        m = self.m\n",
    "        if self.test_run:\n",
    "            print(f'Y : {np.round(Y,3)}')\n",
    "            print(f\"A2 : {np.round(params['A2'], 3)}\")\n",
    "            print(f'shape Y : {Y.shape}')\n",
    "            print(f\"shape A2 : {params['A2'].shape}\")\n",
    "        \n",
    "        params['dZ'+str(len(self.sizes)-1)] = params['A'+str(len(self.sizes)-1)] - Y\n",
    "        params['dW'+str(len(self.sizes)-1)] = (1./m) * params['A'+str(len(self.sizes)-2)].T @ params['dZ'+str(len(self.sizes)-1)]\n",
    "        params['dB'+str(len(self.sizes)-1)] = (1./m) * np.sum(params['dZ'+str(len(self.sizes)-1)], axis=0, keepdims=True)\n",
    "        for i in range(len(self.sizes)-2, 0, -1):\n",
    "            params['dA'+str(i)] = params['dZ'+str(i+1)] @ params['W'+str(i+1)].T\n",
    "            params['dZ'+str(i)] = params['dA'+str(i)] * self.sigmoid_prime(params['Z'+str(i)])\n",
    "            params['dW'+str(i)] = (1./m) * params['A'+str(i-1)].T @ params['dZ'+str(i)]\n",
    "            params['dB'+str(i)] = (1./m) * np.sum(params['dZ'+str(i)], axis=0, keepdims=True)\n",
    "        return \n",
    "    \n",
    "    def __update_network(self):\n",
    "        params = self.params\n",
    "        for i in range(1, len(self.sizes)-1):\n",
    "            params['W'+str(i)] -= self.lr * params['dW'+str(i)]\n",
    "            params['B'+str(i)] -= self.lr * params['dB'+str(i)]\n",
    "        return\n",
    "        \n",
    "    def train_network(self, test_run=True, num_test_runs=1, train_X=None, train_y=None, output_times=True):\n",
    "        times = self.times\n",
    "        self.test_run = test_run\n",
    "        params = self.params\n",
    "        self.J = 0\n",
    "        params['J'] = []\n",
    "        \n",
    "        # quick check for data and warning about loose code\n",
    "        if self.split_data is None:\n",
    "            print('\\n\\ndata error: Data not split, splitting now...')\n",
    "            if (self.X is None) or (self.Y is None):\n",
    "                print('\\ndata error: No Data, please give data with ann.set_data function or rebuid network.')\n",
    "                return -1\n",
    "            else: \n",
    "                self.split(use_stored=True, train_ratio=7, overwrite=True, shuffle=False, pass_back=False)\n",
    "        \n",
    "        for i in range(self.epochs+1):\n",
    "            if i % 10 == 1:\n",
    "                start_time = time.time()\n",
    "            if test_run:\n",
    "                print('\\nwarning: test run started. To train in full set test_run flag to False.')\n",
    "            if train_X is None:\n",
    "                train_X = self.split_data['tx']\n",
    "            if train_y is None:\n",
    "                train_y = self.split_data['ty']\n",
    "            if test_run:\n",
    "                print(f'\\ntrain_X type : {type(train_X)}\\ntrain_X shape : {train_X.shape}')\n",
    "                print(f'\\ntrain_y type : {type(train_X)}\\ntrain_y shape : {train_y.shape}')\n",
    "                print(f'\\ntest_X type : {type(train_X)}\\ntest_X shape : {test_X.shape}')\n",
    "                print(f'\\ntest_y type : {type(train_X)}\\ntest_y shape : {test_y.shape}')\n",
    "            \n",
    "            A2 = self.__forwardpass(train_X)\n",
    "            \n",
    "            if test_run:\n",
    "                print(f'\\n\\nNETWORK TEST RUN PARAMS')\n",
    "                print(f'After Forward Pass\\n')\n",
    "                print('      | KEY |     | SHAPE |\\n')\n",
    "                for k,v in self.params.items():\n",
    "                    print(f'\\t{k}    ',end='')\n",
    "                    try: print(f'        {v.shape}\\n')\n",
    "                    except AttributeError: \n",
    "                        print(f'\\ttype : {type(v)}')\n",
    "                        try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                        except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            \n",
    "            J = self.__get_cost(A2, train_y)\n",
    "            params['J'] += [J]\n",
    "            \n",
    "            self.__backprop(train_y)\n",
    "            \n",
    "            if test_run:\n",
    "                if i == 1:\n",
    "                    print(f'After Backwards Propagation\\n')\n",
    "                    print('      | KEY |     | SHAPE |\\n')\n",
    "                    for k,v in self.params.items():\n",
    "                        print(f'\\t{k}    ',end='')\n",
    "                        try: print(f'        {v.shape}\\n')\n",
    "                        except AttributeError: \n",
    "                            print(f'\\ttype : {type(v)}')\n",
    "                            try: print(f'\\t\\tlen :  {len(v)}\\n')\n",
    "                            except: print(f'\\t\\tvalue : {v}\\n')\n",
    "            self.__update_network()\n",
    "            if i == 0:\n",
    "                print(\"Epoch\", i, \"cost: \", np.round(J, 3))\n",
    "            elif ((i != 0) and (i % 10 == 0)) or test_run:\n",
    "                stop_time = time.time()\n",
    "                epoch_time = np.round((stop_time - start_time)/10,2)\n",
    "                times += [epoch_time]\n",
    "                print(\"Epoch\", i, \"cost: \", np.round(J, 3))\n",
    "                print(f'time per epoch : {epoch_time} seconds')\n",
    "            if test_run:\n",
    "                if i>= num_test_runs:\n",
    "                    return\n",
    "        \n",
    "    def test_network(self, test_X=None, test_y=None):\n",
    "        if test_X is None:\n",
    "            test_X = self.split_data['Tx']\n",
    "        if test_y is None:\n",
    "            test_y = self.split_data['Ty']\n",
    "        A2 = self.__forwardpass(test_X)\n",
    "        score = 0\n",
    "        for yhat,y in zip(A2,test_y):\n",
    "            if np.argmax(yhat) == np.argmax(y):\n",
    "                score+=1\n",
    "        accuracy = score/len(test_y)\n",
    "        print(f'test acc = {np.round(accuracy,2)*100}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53767b66-2d38-46cf-9996-e9c87534a930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it wasnt a thing.\n"
     ]
    }
   ],
   "source": [
    "try: del(nnp)\n",
    "except NameError: print('it wasnt a thing.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e922678-239f-4875-b731-e70643b90633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "data error: Data not split, splitting now...\n",
      "Epoch 0 cost:  6.097\n",
      "Epoch 10 cost:  0.874\n",
      "time per epoch : 1.89 seconds\n",
      "Epoch 20 cost:  0.541\n",
      "time per epoch : 1.78 seconds\n",
      "Epoch 30 cost:  0.45\n",
      "time per epoch : 1.71 seconds\n",
      "Epoch 40 cost:  0.399\n",
      "time per epoch : 1.73 seconds\n",
      "Epoch 50 cost:  0.365\n",
      "time per epoch : 1.74 seconds\n",
      "Epoch 60 cost:  0.339\n",
      "time per epoch : 1.75 seconds\n",
      "Epoch 70 cost:  0.319\n",
      "time per epoch : 1.76 seconds\n",
      "Epoch 80 cost:  0.303\n",
      "time per epoch : 1.76 seconds\n",
      "Epoch 90 cost:  0.289\n",
      "time per epoch : 1.76 seconds\n",
      "Epoch 100 cost:  0.277\n",
      "time per epoch : 1.76 seconds\n",
      "Epoch 110 cost:  0.267\n",
      "time per epoch : 1.77 seconds\n",
      "Epoch 120 cost:  0.258\n",
      "time per epoch : 1.76 seconds\n",
      "Epoch 130 cost:  0.25\n",
      "time per epoch : 1.77 seconds\n",
      "Epoch 140 cost:  0.243\n",
      "time per epoch : 1.79 seconds\n",
      "Epoch 150 cost:  0.236\n",
      "time per epoch : 1.77 seconds\n"
     ]
    }
   ],
   "source": [
    "nnp = ann_np(sizes=[784, 128, 10], rate=10, epochs=150, X=X, Y=Y)\n",
    "nnp.split(use_stored=True, pass_back=False)\n",
    "nnp.train_network(test_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d3dc569-651d-40f1-abc6-05feee1b1f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc = 93.0\n"
     ]
    }
   ],
   "source": [
    "nnp.test_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb8cbde",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fig, axs \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[39m# set background to black\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig\u001b[39m.\u001b[39mpatch\u001b[39m.\u001b[39mset_facecolor(\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,4))\n",
    "_=axs[0].plot(nnp.times)\n",
    "_=axs[1].plot(nnp.params['J'])\n",
    "_=axs[0].plot(nn.times)\n",
    "_=axs[1].plot(nn.params['J'])\n",
    "_=axs[0].set_title('time per epoch')\n",
    "_=axs[1].set_title('cost per epoch')\n",
    "_=axs[0].set_xlabel('epoch')\n",
    "_=axs[1].set_xlabel('epoch')\n",
    "_=axs[0].set_ylabel('time (s)')\n",
    "_=axs[1].set_ylabel('cost')\n",
    "_=axs[0].grid(alpha=0.5)\n",
    "_=axs[1].grid(alpha=0.5)\n",
    "_=axs[0].legend(['np -> cpu','tnp -> gpu'], loc='center right')\n",
    "_=axs[1].legend(['np -> cpu','tnp -> gpu'], loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ed6cc90",
   "metadata": {},
   "source": [
    "$$ V_{dq} = 0, Sdw = 0, Vab = 0, Sab = 0 $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2",
   "language": "python",
   "name": "data2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "6a5edf34ff94fe8ee16edcf4bc6701972d1829ecc28373df8053e0b2510fc999"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
