{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                       | 0/10 [01:00<?, ?epoch/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.experimental.numpy as tnp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import struct\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "\n",
    "\n",
    "def load_mnist(images_path, labels_path):\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), rows, cols)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "train_images_path = 'train-images-idx3-ubyte'\n",
    "train_labels_path = 'train-labels-idx1-ubyte'\n",
    "test_images_path = 't10k-images-idx3-ubyte'\n",
    "test_labels_path = 't10k-labels-idx1-ubyte'\n",
    "\n",
    "train_images, train_labels = load_mnist(train_images_path, train_labels_path)\n",
    "test_images, test_labels = load_mnist(test_images_path, test_labels_path)\n",
    "\n",
    "def preprocess_data(images):\n",
    "    images = tnp.array(images, dtype=tnp.float32) / 255.0\n",
    "    images = tnp.expand_dims(images, axis=-1)\n",
    "    return images\n",
    "\n",
    "train_images = preprocess_data(train_images)\n",
    "test_images = preprocess_data(test_images)\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "    return tnp.array(np.eye(10)[labels])\n",
    "\n",
    "train_labels = one_hot_encoding(train_labels)\n",
    "test_labels = one_hot_encoding(test_labels)\n",
    "\n",
    "def conv2d(input_data, filters):\n",
    "    num_filters, num_channels, filter_size, _ = filters.shape\n",
    "    batch_size, input_height, input_width, _ = input_data.shape\n",
    "    output_height = input_height - filter_size + 1\n",
    "    output_width = input_width - filter_size + 1\n",
    "\n",
    "    output_list = []\n",
    "    for i in range(num_filters):\n",
    "        filter_output = []\n",
    "        for j in range(output_height):\n",
    "            for k in range(output_width):\n",
    "                patch_sum = tnp.sum(input_data[:, j:j+filter_size, k:k+filter_size, :] * filters[i], axis=(1, 2, 3))\n",
    "                filter_output.append(patch_sum)\n",
    "        output_list.append(tnp.stack(filter_output).reshape(batch_size, output_height, output_width))\n",
    "\n",
    "    return tnp.stack(output_list, axis=1)\n",
    "\n",
    "def max_pooling2d(input_data):\n",
    "    batch_size, num_filters, input_height, input_width = input_data.shape\n",
    "    output_height, output_width = input_height // 2, input_width // 2  # assuming 2x2 pooling with stride of 2\n",
    "    output = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        batch_output = []\n",
    "        for j in range(num_filters):\n",
    "            filter_output = []\n",
    "            for k in range(output_height):\n",
    "                row_output = []\n",
    "                for l in range(output_width):\n",
    "                    row_output.append(tnp.max(input_data[i, j, 2*k:2*(k+1), 2*l:2*(l+1)]))\n",
    "                filter_output.append(row_output)\n",
    "            batch_output.append(filter_output)\n",
    "        output.append(batch_output)\n",
    "\n",
    "    return tnp.array(output)\n",
    "\n",
    "def fully_connected(input_data, weights, biases):\n",
    "    return tnp.dot(input_data, weights) + biases\n",
    "\n",
    "def relu(input_data):\n",
    "    return tnp.maximum(input_data, 0)\n",
    "\n",
    "def softmax(input_data):\n",
    "    exp_data = tnp.exp(input_data - tnp.max(input_data, axis=-1, keepdims=True))\n",
    "    return exp_data / tnp.sum(exp_data, axis=-1, keepdims=True)\n",
    "\n",
    "def forward_propagation(input_data):\n",
    "    conv1 = relu(conv2d(input_data, conv1_filters))\n",
    "    pool1 = max_pooling2d(conv1)\n",
    "    conv2 = relu(conv2d(pool1, conv2_filters))\n",
    "    pool2 = max_pooling2d(conv2)\n",
    "    \n",
    "    flattened = pool2.reshape(pool2.shape[0], -1)\n",
    "    dense1 = relu(fully_connected(flattened, dense1_weights, dense1_biases))\n",
    "    dense2 = relu(fully_connected(dense1, dense2_weights, dense2_biases))\n",
    "    return softmax(fully_connected(dense2, dense3_weights, dense3_biases))\n",
    "\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    return -tnp.sum(labels * tnp.log(predictions)) / labels.shape[0]\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return tnp.mean(tnp.argmax(predictions, axis=-1) == tnp.argmax(labels, axis=-1))\n",
    "\n",
    "def calculate_gradients(input_data, labels):\n",
    "    # TODO: implement backpropagation here to calculate gradients\n",
    "    pass\n",
    "\n",
    "# Initialize the weights and biases\n",
    "conv1_filters = tf.random.normal((6, 1, 5, 5), 0, 0.1)  # 6 filters of size 5x5 with 1 input channel\n",
    "conv2_filters = tf.random.normal((16, 6, 5, 5), 0, 0.1)  # 16 filters of size 5x5 with 6 input channels\n",
    "dense1_weights = tf.random.normal((400, 120), 0, 0.1)  # 120 nodes with 400 input nodes\n",
    "dense2_weights = tf.random.normal((120, 84), 0, 0.1)  # 84 nodes with 120 input nodes\n",
    "dense3_weights = tf.random.normal((84, 10), 0, 0.1)  # 10 nodes with 84 input nodes\n",
    "\n",
    "dense1_biases = np.zeros((120,))\n",
    "dense2_biases = np.zeros((84,))\n",
    "dense3_biases = np.zeros((10,))\n",
    "\n",
    "# convert these numpy arrays back to tensorflow tensors for the computations\n",
    "conv1_filters = tf.convert_to_tensor(conv1_filters, dtype=tf.float32)\n",
    "conv2_filters = tf.convert_to_tensor(conv2_filters, dtype=tf.float32)\n",
    "dense1_weights = tf.convert_to_tensor(dense1_weights, dtype=tf.float32)\n",
    "dense2_weights = tf.convert_to_tensor(dense2_weights, dtype=tf.float32)\n",
    "dense3_weights = tf.convert_to_tensor(dense3_weights, dtype=tf.float32)\n",
    "dense1_biases = tf.convert_to_tensor(dense1_biases, dtype=tf.float32)\n",
    "dense2_biases = tf.convert_to_tensor(dense2_biases, dtype=tf.float32)\n",
    "dense3_biases = tf.convert_to_tensor(dense3_biases, dtype=tf.float32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "pbar = tqdm(total=epochs, desc='Training', unit='epoch', ncols=80)\n",
    "\n",
    "learning_rate = 0.01\n",
    "for epoch in range(epochs):\n",
    "    outputs = forward_propagation(train_images)\n",
    "    loss = cross_entropy_loss(outputs, train_labels)\n",
    "    acc = accuracy(outputs, train_labels)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss}, Accuracy: {acc}')\n",
    "    # Update progress bar\n",
    "    pbar.update(1)\n",
    "    # TODO: Use the gradients to update the weights and biases here\n",
    "\n",
    "# Close progress bar\n",
    "pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2",
   "language": "python",
   "name": "data2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
