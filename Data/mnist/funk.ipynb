{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.2470 - accuracy: 0.9259\n",
      "Epoch 2/2\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0917 - accuracy: 0.9718\n",
      "Test loss: 0.0669\n",
      "Test accuracy: 0.9790\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy.linalg' has no attribute 'expm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m---> 31\u001b[0m     model\u001b[39m.\u001b[39;49mupdate(X, y, \u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m, in \u001b[0;36mMyModel.update\u001b[0;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[1;32m     22\u001b[0m grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient(X, y)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Here we use the matrix exponential and logarithm, which come from the theory of Lie groups.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters, np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mexpm(\u001b[39m-\u001b[39mlearning_rate \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mlogm(grad)))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy.linalg' has no attribute 'expm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "        # Initialize parameters as an identity matrix (an element of the special orthogonal group SO(3)).\n",
    "        self.parameters = np.eye(3)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Use the parameters in some way to predict the labels for X.\n",
    "        pass\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Compute the loss between the predictions and the actual labels.\n",
    "        pass\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        # Compute the gradient of the loss with respect to the parameters.\n",
    "        pass\n",
    "\n",
    "    def update(self, X, y, learning_rate):\n",
    "        # Perform a gradient descent step in the space of the Lie group.\n",
    "        grad = self.gradient(X, y)\n",
    "        # Here we use the matrix exponential and logarithm, which come from the theory of Lie groups.\n",
    "        self.parameters = np.dot(self.parameters, np.linalg.expm(-learning_rate * np.linalg.logm(grad)))\n",
    "\n",
    "model = MyModel()\n",
    "X = np.random.rand(100, 3)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "for _ in range(1000):\n",
    "    model.update(X, y, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected square array_like input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand(\u001b[39m100\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1000\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     model\u001b[39m.\u001b[39;49mupdate(X, y, \u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[28], line 25\u001b[0m, in \u001b[0;36mMyModel.update\u001b[0;34m(self, X, y, learning_rate)\u001b[0m\n\u001b[1;32m     23\u001b[0m grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgradient(X, y)\n\u001b[1;32m     24\u001b[0m \u001b[39m# Here we use the matrix exponential and logarithm, which come from the theory of Lie groups.\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters, expm(\u001b[39m-\u001b[39mlearning_rate \u001b[39m*\u001b[39m logm(grad)))\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/data2/lib/python3.11/site-packages/scipy/linalg/_matfuncs.py:199\u001b[0m, in \u001b[0;36mlogm\u001b[0;34m(A, disp)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlogm\u001b[39m(A, disp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m    Compute matrix logarithm.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \n\u001b[1;32m    198\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     A \u001b[39m=\u001b[39m _asarray_square(A)\n\u001b[1;32m    200\u001b[0m     \u001b[39m# Avoid circular import ... this is OK, right?\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_matfuncs_inv_ssq\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/data2/lib/python3.11/site-packages/scipy/linalg/_matfuncs.py:56\u001b[0m, in \u001b[0;36m_asarray_square\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m     54\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(A)\n\u001b[1;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(A\u001b[39m.\u001b[39mshape) \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m A\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]:\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mexpected square array_like input\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[39mreturn\u001b[39;00m A\n",
      "\u001b[0;31mValueError\u001b[0m: expected square array_like input"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm, logm\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self):\n",
    "        # Initialize parameters as an identity matrix (an element of the special orthogonal group SO(3)).\n",
    "        self.parameters = np.eye(3)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Use the parameters in some way to predict the labels for X.\n",
    "        pass\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Compute the loss between the predictions and the actual labels.\n",
    "        pass\n",
    "\n",
    "    def gradient(self, X, y):\n",
    "        # Compute the gradient of the loss with respect to the parameters.\n",
    "        pass\n",
    "\n",
    "    def update(self, X, y, learning_rate):\n",
    "        # Perform a gradient descent step in the space of the Lie group.\n",
    "        grad = self.gradient(X, y)\n",
    "        # Here we use the matrix exponential and logarithm, which come from the theory of Lie groups.\n",
    "        self.parameters = np.dot(self.parameters, expm(-learning_rate * logm(grad)))\n",
    "\n",
    "model = MyModel()\n",
    "X = np.random.rand(100, 3)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "for _ in range(1000):\n",
    "    model.update(X, y, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data2",
   "language": "python",
   "name": "data2"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
